[{"path":"https://cmatkhan.github.io/BSA/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to BSA","title":"Contributing to BSA","text":"outlines propose change BSA. detailed info contributing , tidyverse packages, please see development contributing guide.","code":""},{"path":"https://cmatkhan.github.io/BSA/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to BSA","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"https://cmatkhan.github.io/BSA/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to BSA","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed).","code":""},{"path":"https://cmatkhan.github.io/BSA/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to BSA","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"cmatKhan/BSA\", fork = TRUE). Install development dependencies devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"https://cmatkhan.github.io/BSA/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to BSA","text":"New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"https://cmatkhan.github.io/BSA/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to BSA","text":"Please note BSA project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://cmatkhan.github.io/BSA/SUPPORT.html","id":null,"dir":"","previous_headings":"","what":"Getting help with BSA","title":"Getting help with BSA","text":"Thanks using BSA! filing issue, places explore pieces put together make process smooth possible.","code":""},{"path":"https://cmatkhan.github.io/BSA/SUPPORT.html","id":"make-a-reprex","dir":"","previous_headings":"","what":"Make a reprex","title":"Getting help with BSA","text":"Start making minimal reproducible example using reprex package. haven’t heard used reprex , ’re treat! Seriously, reprex make R-question-asking endeavors easier (pretty insane ROI five ten minutes ’ll take learn ’s ). additional reprex pointers, check Get help! section tidyverse site.","code":""},{"path":"https://cmatkhan.github.io/BSA/SUPPORT.html","id":"where-to-ask","dir":"","previous_headings":"","what":"Where to ask?","title":"Getting help with BSA","text":"Armed reprex, next step figure ask. See also Bioconductor help website. ’s question: start community.rstudio.com, /StackOverflow. Bioconductor-related question, please ask Bioconductor Support Website using appropriate package tag (website send automatic email package authors). people answer questions. ’s bug: ’re right place, file issue. ’re sure: let community help figure ! problem bug feature request, can easily return report . opening new issue, sure search issues pull requests make sure bug hasn’t reported /already fixed development version. default, search pre-populated :issue :open. can edit qualifiers (e.g. :pr, :closed) needed. example, ’d simply remove :open search issues repo, open closed.","code":""},{"path":"https://cmatkhan.github.io/BSA/SUPPORT.html","id":"what-happens-next","dir":"","previous_headings":"","what":"What happens next?","title":"Getting help with BSA","text":"efficient possible, development tidyverse packages tends bursty, shouldn’t worry don’t get immediate response. Typically don’t look repo sufficient quantity issues accumulates, ’s burst intense activity focus efforts. makes development efficient avoids expensive context switching problems, cost taking longer get back . process makes good reprex particularly important might multiple months initial report start working . can’t reproduce bug, can’t fix !","code":""},{"path":"https://cmatkhan.github.io/BSA/articles/BSA2.html","id":"gather-data","dir":"Articles","previous_headings":"","what":"Gather data","title":"BSA2","text":"","code":"##### Parent strains - parents <- list() for (x in c(\"KN99a\", \"TDY1993\")) {   parents[[x]] <- read.table(file = paste0(\"/mnt/lts/personal/chasem/DNA/EXP#067/VCFtables/\", x, \".txt\"), header = T, stringsAsFactors = F, numerals = \"allow.loss\")   parents[[x]]$Alternative1 <- as.integer(parents[[x]]$Alternative1) } # Finding the positions where KN99a is reference and TDY1993 (C8) is alternative. parents <- parents[[1]][parents[[2]]$Filt_Genotype == \"Alternative\" & parents[[1]]$Filt_Genotype == \"Reference\", ]  poolsBSA2 <- list.files(path = \"/mnt/lts/personal/chasem/DNA/EXP#031/VCF_tables\") %>%   sub(pattern = \".txt\", replacement = \"\")  mySamples <- list() for (i in poolsBSA2) {   mySamples[[i]] <- read.table(file = paste0(     \"/mnt/lts/personal/chasem/DNA/EXP#031/VCF_tables/\", i,     \".txt\"   ), header = T, stringsAsFactors = F, numerals = \"allow.loss\")   mySamples[[i]] <- mySamples[[i]][paste(mySamples[[i]]$CHR, mySamples[[i]]$POS, sep = \"_\") %in% paste(parents$CHR, parents$POS, sep = \"_\"), ]   mySamples[[i]]$Alternative1 <- as.integer(mySamples[[i]]$Alternative1) }"},{"path":"https://cmatkhan.github.io/BSA/articles/BSA2.html","id":"summary-stats","dir":"Articles","previous_headings":"","what":"Summary stats","title":"BSA2","text":"","code":"# Summary of data from the coverage SampleMean <- sapply(mySamples, FUN = function(x) {   mean(x$RealDepth) }) SampleMedian <- sapply(mySamples, FUN = function(x) {   median(x$RealDepth) })"},{"path":"https://cmatkhan.github.io/BSA/articles/BSA2.html","id":"separate-samplestissues","dir":"Articles","previous_headings":"","what":"separate samples/tissues","title":"BSA2","text":"","code":"pools <- c(paste0(\"P\", c(1:5), \".1\"), paste0(\"P\", c(1:5), \".2\")) # The different pools, in this case, P1 to P5 and all Inoculum <- grep(pattern = \"P[[:alnum:]].{1,3}I\", poolsBSA2, value = T) # The Inoculum samples YPD <- grep(pattern = \"P[[:alnum:]].{1,3}Y\", poolsBSA2, value = T) # The YPD samples Lungs <- grep(pattern = \"P[[:alnum:]].{1,3}L\", poolsBSA2, value = T) # The Lung samples Brains <- grep(pattern = \"P[[:alnum:]].{1,3}B\", poolsBSA2, value = T) # The Brain samples"},{"path":"https://cmatkhan.github.io/BSA/articles/BSA2.html","id":"replicatesbsa","dir":"Articles","previous_headings":"","what":"replicatesBSA","title":"BSA2","text":"","code":"replicatesBSA <- list() for (i in pools) {   for (z in grep(pattern = i, x = c(YPD, Lungs, Brains), value = T)) {     lowbulk <- grep(pattern = i, x = Inoculum, value = T)     highbulk <- z     replicatesBSA[[highbulk]] <- picker2(mySamples, lowBulk = lowbulk, highBulk = highbulk)     print(paste(\"Comparisons made:\", highbulk, \"x\", lowbulk, sep = \" \"))   } }"},{"path":"https://cmatkhan.github.io/BSA/articles/BSA2.html","id":"combine-replicates","dir":"Articles","previous_headings":"","what":"combine replicates","title":"BSA2","text":"","code":"# Making a new list with the sums of all replicates of a pool  summedPoolsBSA <- list() # a list with the summed depth and allele counts for all replicates of a specific pool poolsBSA <- list() # a list of comparisons of pooled replicates (all the replicates from a specific pool) for each condition using picker() pools <- paste0(\"P\", 1:5)  for (i in pools) {   # Inoculum   # # this is the basis for the table; the four first columns of mySamples.   meta <- mySamples[[grep(pattern = i, x = Inoculum, value = T)[1]]][, 1:4]    meta$RealDepth <- rowSums(     sapply(grep(pattern = i,                  x = Inoculum,                  value = T),             FUN = function(x) {               cbind(mySamples[[x]][, 8])})     )      meta$Reference <- rowSums(sapply(grep(pattern = i, x = Inoculum, value = T), FUN = function(x) {     cbind(mySamples[[x]][, 9])   }))   meta$Alternative1 <- rowSums(sapply(grep(pattern = i, x = Inoculum, value = T), FUN = function(x) {     cbind(mySamples[[x]][, 10])   }))   summedPoolsBSA[[paste0(i, \"I\")]] <- meta   rm(meta)    # YPD   meta <- mySamples[[grep(pattern = i, x = YPD, value = T)[1]]][, 1:4] # this is the basis for the table; the four first columns of mySamples.   meta$RealDepth <- rowSums(sapply(grep(pattern = i, x = YPD, value = T), FUN = function(x) {     cbind(mySamples[[x]][, 8])   }))   meta$Reference <- rowSums(sapply(grep(pattern = i, x = YPD, value = T), FUN = function(x) {     cbind(mySamples[[x]][, 9])   }))   meta$Alternative1 <- rowSums(sapply(grep(pattern = i, x = YPD, value = T), FUN = function(x) {     cbind(mySamples[[x]][, 10])   }))   summedPoolsBSA[[paste0(i, \"Y\")]] <- meta   rm(meta)    # Lungs   meta <- mySamples[[grep(pattern = i, x = Lungs, value = T)[1]]][, 1:4] # this is the basis for the table; the four first columns of mySamples.   meta$RealDepth <- rowSums(sapply(grep(pattern = i, x = Lungs, value = T), FUN = function(x) {     cbind(mySamples[[x]][, 8])   }))   meta$Reference <- rowSums(sapply(grep(pattern = i, x = Lungs, value = T), FUN = function(x) {     cbind(mySamples[[x]][, 9])   }))   meta$Alternative1 <- rowSums(sapply(grep(pattern = i, x = Lungs, value = T), FUN = function(x) {     cbind(mySamples[[x]][, 10])   }))   summedPoolsBSA[[paste0(i, \"L\")]] <- meta   rm(meta)    meta <- mySamples[[grep(pattern = i, x = Brains, value = T)[1]]][, 1:4] # this is the basis for the table; the four first columns of mySamples.   meta$RealDepth <- rowSums(sapply(grep(pattern = i, x = Brains, value = T), FUN = function(x) {     cbind(mySamples[[x]][, 8])   }))   meta$Reference <- rowSums(sapply(grep(pattern = i, x = Brains, value = T), FUN = function(x) {     cbind(mySamples[[x]][, 9])   }))   meta$Alternative1 <- rowSums(sapply(grep(pattern = i, x = Brains, value = T), FUN = function(x) {     cbind(mySamples[[x]][, 10])   }))   summedPoolsBSA[[paste0(i, \"B\")]] <- meta   rm(meta) }"},{"path":"https://cmatkhan.github.io/BSA/articles/BSA2.html","id":"pooled-bsa","dir":"Articles","previous_headings":"","what":"pooled BSA","title":"BSA2","text":"","code":"## Making the contrasts of low and high bulk with the summed pools  for (i in pools) { # The pools   for (y in c(\"Y\", \"L\", \"B\")) {     lowbulk <- paste0(i, \"I\")     highbulk <- paste0(i, y)     if (lowbulk %in% names(summedPoolsBSA) & highbulk %in% names(summedPoolsBSA)) {       poolsBSA[[highbulk]] <- picker2(summedPoolsBSA, lowBulk = lowbulk, highBulk = highbulk)       print(paste(\"Comparisons made:\", highbulk, \"x\", lowbulk, sep = \" \"))     }   } }  # Here we start summing all pools together for each condition, and then making the comparisons.  allPoolsInOneBSA <- list() # This will be the list containing three itens, the sum of all inocula, all YPDs and all Inoculum. allPoolsInOneComparisonBSA <- list()  for (y in c(\"I\", \"Y\", \"L\", \"B\")) { # This loop goes around for each condition.   allPoolsInOneBSA[[y]] <- summedPoolsBSA[[1]][, 1:4] # The first four columns are just the information about each mutation, such as CHR and Position...   cond <- grep(y, names(summedPoolsBSA), value = T)   allPoolsInOneBSA[[y]]$RealDepth <- apply(sapply(summedPoolsBSA[cond], function(x) x$RealDepth), 1, sum) # This is another way of doing it (see line 53)   allPoolsInOneBSA[[y]]$Reference <- apply(sapply(summedPoolsBSA[cond], function(x) x$Reference), 1, sum)   allPoolsInOneBSA[[y]]$Alternative1 <- apply(sapply(summedPoolsBSA[cond], function(x) x$Alternative1), 1, sum) }  # This will analyze the comparisons between the conditions of the allPoolsInOne object  for (i in c(\"Y\", \"L\", \"B\")) {   lowbulk <- \"I\"   highbulk <- i   allPoolsInOneComparisonBSA[[i]] <- picker2(allPoolsInOneBSA, lowBulk = lowbulk, highBulk = highbulk)   print(paste(\"Comparisons made:\", highbulk, \"x\", lowbulk, sep = \" \")) }  ### FILTERING  # replicates_filteredBSA=list() pools_filteredBSA <- list() allPoolsInOne_filteredBSA <- list()   for (i in c(names(poolsBSA)[c(-3, -6, -9, -12, -15)], names(poolsBSA)[c(3, 6, 9, 12, 15)])) {   print(i)   pools_filteredBSA[[i]] <- analyzer(poolsBSA[[i]][!is.nan(poolsBSA[[i]]$deltaSNP), ],     minDepthPercentile = 0.005,     maxDepthPercentile = 0.9, outlierFilt = \"deltaSNP\"   ) }  for (i in names(allPoolsInOneComparisonBSA)) {   print(i)   allPoolsInOne_filteredBSA[[i]] <- analyzer(allPoolsInOneComparisonBSA[[i]][!is.nan(allPoolsInOneComparisonBSA[[i]]$deltaSNP), ],     minDepthPercentile = 0.005, maxDepthPercentile = 0.995, outlierFilt = \"deltaSNP\"   ) }  #save.image(file = here(\"data/BSA2_reproduced_20220804.RData\"))"},{"path":"https://cmatkhan.github.io/BSA/articles/BSA2_plots.html","id":"bin-information","dir":"Articles","previous_headings":"","what":"Bin information","title":"BSA2_plots","text":"rather table per variant, table per bin. rows bins, columns columns found binner2 end","code":"### PLOTS  chosenSize <- 5000 Pools <- list() allPools <- list()  ptm <- proc.time() no_cores <- detectCores()-1 # length(names(pools_filteredBSA))  cl <- makeCluster(no_cores)  # export data, functions and variables to cluster clusterExport(cl, c(\"pools_filteredBSA\",                     \"binner2\",                      \"Pools\",                      \"allPools\",                      \"chosenSize\"))  # Calling the binner2 in parallel Pools <- parLapply(   cl, names(pools_filteredBSA),   function(x) {     # make sure this is documented     binner2(pools_filteredBSA[[       grep(x,             names(pools_filteredBSA))]],        bin.size = chosenSize)   } ) stopCluster(cl) # Closing the parallel cluster proc.time() - ptm for (i in names(allPoolsInOne_filteredBSA)) {   allPools[[i]] <- binner2(allPoolsInOne_filteredBSA[[i]],     bin.size = chosenSize   ) }  names(Pools) <- names(pools_filteredBSA) names(allPools) <- names(allPoolsInOne_filteredBSA) condition <- c(\"YPD\", \"Lungs\") ###### For plots with smoothed Delta SNP ##### samples <- list() samples <- lapply(c(1:3), FUN = function(x) {   data.frame(     CHROM = allPools[[names(allPools)[x]]]$CHROM,     binFloor = allPools[[names(allPools)[x]]]$binFloor,     binCeiling = allPools[[names(allPools)[x]]]$binCeiling,     binMiddle = allPools[[names(allPools)[x]]]$binMiddle,     Bin_size = allPools[[names(allPools)[x]]]$Bin_size,     P1 = Pools[[paste0(\"P1\", names(allPools)[x])]]$smoothedDeltaSNP,     P1sig = Pools[[paste0(\"P1\", names(allPools)[x])]]$Significance,     P2 = Pools[[paste0(\"P2\", names(allPools)[x])]]$smoothedDeltaSNP,     P2sig = Pools[[paste0(\"P2\", names(allPools)[x])]]$Significance,     P3 = Pools[[paste0(\"P3\", names(allPools)[x])]]$smoothedDeltaSNP,     P3sig = Pools[[paste0(\"P3\", names(allPools)[x])]]$Significance,     P4 = Pools[[paste0(\"P4\", names(allPools)[x])]]$smoothedDeltaSNP,     P4sig = Pools[[paste0(\"P4\", names(allPools)[x])]]$Significance,     P5 = Pools[[paste0(\"P5\", names(allPools)[x])]]$smoothedDeltaSNP,     P5sig = Pools[[paste0(\"P5\", names(allPools)[x])]]$Significance,     All_pools = allPools[[names(allPools)[x]]]$smoothedDeltaSNP,     mut_perBin = allPools[[names(allPools)[x]]]$mut_perBin,     Significance = allPools[[names(allPools)[x]]]$Significance   ) %>%     mutate(       pool_max = pmax(P1, P2, P3, P4, P5, na.rm = T),       pool_min = pmin(P1, P2, P3, P4, P5, na.rm = T),       Condition = rep(condition[x], nrow(allPools[[names(allPools)[x]]]))     ) }) names(samples) <- names(allPools)[c(1:3)] ###### For plots with NON-smoothed Delta SNP ##### samplesNonSmooth <- list() samplesNonSmooth <- lapply(c(1:3), FUN = function(x) {   data.frame(     CHROM = allPools[[names(allPools)[x]]]$CHROM,     binFloor = allPools[[names(allPools)[x]]]$binFloor,     binCeiling = allPools[[names(allPools)[x]]]$binCeiling,     binMiddle = allPools[[names(allPools)[x]]]$binMiddle,     Bin_size = allPools[[names(allPools)[x]]]$Bin_size,     P1 = Pools[[paste0(\"P1\", names(allPools)[x])]]$smoothedDeltaSNP,     P1sig = Pools[[paste0(\"P1\", names(allPools)[x])]]$Significance,     P2 = Pools[[paste0(\"P2\", names(allPools)[x])]]$smoothedDeltaSNP,     P2sig = Pools[[paste0(\"P2\", names(allPools)[x])]]$Significance,     P3 = Pools[[paste0(\"P3\", names(allPools)[x])]]$smoothedDeltaSNP,     P3sig = Pools[[paste0(\"P3\", names(allPools)[x])]]$Significance,     P4 = Pools[[paste0(\"P4\", names(allPools)[x])]]$smoothedDeltaSNP,     P4sig = Pools[[paste0(\"P4\", names(allPools)[x])]]$Significance,     P5 = Pools[[paste0(\"P5\", names(allPools)[x])]]$smoothedDeltaSNP,     P5sig = Pools[[paste0(\"P5\", names(allPools)[x])]]$Significance,     All_pools = allPools[[names(allPools)[x]]]$deltaSNP,     mut_perBin = allPools[[names(allPools)[x]]]$mut_perBin,     Significance = allPools[[names(allPools)[x]]]$Significance   ) %>%     mutate(       pool_max = pmax(P1, P2, P3, P4, P5, na.rm = T),       pool_min = pmin(P1, P2, P3, P4, P5, na.rm = T),       Condition = rep(condition[x], nrow(allPools[[names(allPools)[x]]]))     ) })  names(samplesNonSmooth) <- names(allPools)[c(1:3)] # This is a table with the Max and Min of ∆SNP for each position. MaxAndMins <- cbind(allPoolsInOne_filteredBSA[[\"Y\"]][, 1:4], data.frame(   Min_SNPchange = -(allPoolsInOne_filteredBSA[[\"Y\"]]$SNPindex.LOW),   Max_SNPchange = 1 - (allPoolsInOne_filteredBSA[[\"Y\"]]$SNPindex.LOW) )) # The SNPindex.LOW should be the same for YPD and Lungs....   MaxAndMins <- MaxAndMins %>% filter(!(CHROM == \"chr2\" & (POS == 283162 | POS == 283652 | (POS > 466000 & POS < 467000)))) # This mutation was erased in the addition of the drug marker in the C8 parent (It was in the arm). # PLOTS # The IRs IR_list <- list() IR_list[[\"chr1\"]] <- c(1262.121, 1340.862) IR_list[[\"chr2\"]] <- c(283.162, 468.855) IR_list[[\"chr3\"]] <- c(15.943, 117.260) IR_list[[\"chr7\"]] <- c(19.297, 146.051) IR_list[[\"chr8\"]] <- c(13.998, 55.13) # Defining temp, a subset with only the chosen chromosome.  dataToUse <- samplesNonSmooth for (i in 1:14) {   rm(temp)   rm(PoolsCombine)   rm(newMinsAndMax)    temp <- rbind(     dataToUse[[\"L\"]][dataToUse[[\"L\"]]$CHROM == paste0(\"chr\", i), ],     dataToUse[[\"Y\"]][dataToUse[[\"Y\"]]$CHROM == paste0(\"chr\", i), ]   )   temp$combine <- paste(temp$Significance, temp$Condition, sep = \"_\")   levels(temp$combine) <- c(\"FALSE_Inoculum\", \"FALSE_YPD\", \"TRUE_Inoculum\", \"TRUE_YPD\")   eachPool <- c(\"P1sig\", \"P2sig\", \"P3sig\", \"P4sig\", \"P5sig\")   PoolsCombine <- lapply(eachPool, FUN = function(x) {     z <- paste(temp[, grep(x, colnames(temp))], temp$Condition, sep = \"_\")     return(z)   })    newMinsAndMax <- MaxAndMins %>% filter(CHROM == paste0(\"chr\", i))    p1 <- ggplot(data = temp, aes(x = temp$binMiddle / 1000, y = temp$All_pools)) +     geom_line(data = newMinsAndMax, aes(x = POS / 1000, y = Min_SNPchange)) +     geom_line(data = newMinsAndMax, aes(x = POS / 1000, y = Max_SNPchange)) +     geom_pointrange(       data = temp,       aes(         ymin = pool_min,         ymax = pool_max,         x = binMiddle / 1000,         y = All_pools,         size = combine,         colour = combine,         alpha = combine,         order = \"combine\"       )     ) +     scale_size_manual(       name = \"Condition and significance\",       labels = c(\"Lung, not significant\", \"YPD, not significant\", \"Lung, significant\", \"YPD, significant\"),       values = c(0.1, 0.1, 0.4, 0.4), drop = F     ) +     scale_colour_manual(       name = \"Condition and significance\",       labels = c(\"Lung, not significant\", \"YPD, not significant\", \"Lung, significant\", \"YPD, significant\"),       values = c(         \"FALSE_Lungs\" = \"red\", \"FALSE_YPD\" = \"black\",         \"TRUE_Lungs\" = \"red\", \"TRUE_YPD\" = \"black\"       ), drop = F     ) +     scale_alpha_manual(       name = \"Condition and significance\",       labels = c(\"Lung, not significant\", \"YPD, not significant\", \"Lung, significant\", \"YPD, significant\"),       values = c(0.95, 0.95, 1, 1), drop = F     ) +     # geom_vline(xintercept  = IR_list[[paste0(\"chr\",i)]],color=\"black\", size=0.3, linetype=\"dashed\")+     # geom_vline(xintercept = 0, size=0.3)+     labs(title = paste0(\"Chromosome \", i), x = paste0(\"Position on chromosome \", i, \" (kb)\"), y = \"Change in allele frequency (BSA)\", color = \"Pool\") +     geom_hline(yintercept = c(0), color = \"black\", size = 0.6) +     scale_x_continuous(breaks = seq(from = 0, to = 2400, by = 300), limits = c(0, 2400), expand = c(0, 0)) +     scale_y_continuous(breaks = c(-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1), limits = c(-1.05, 1.05), expand = c(0, 0)) +     theme( # legend.title = element_text(size=18),       legend.position = \"right\",       plot.title = element_blank(),       legend.text = element_text(size = 16),       axis.title = element_text(size = 32),       axis.title.y = element_text(margin = margin(r = 25, t = 0, l = 5, b = 0)),       axis.title.x = element_text(margin = margin(r = 0, t = 25, l = 0, b = 5)),       axis.text = element_text(size = 28),       axis.ticks.length = unit(0.3, \"cm\"),       panel.grid.major = element_blank(),       panel.grid.minor = element_blank(),       panel.background = element_rect(fill = \"white\", colour = \"black\", size = 1),       text = element_text(family = \"sans\")     )    ggsave(filename = file.path(plot_out_prefix,paste0(\"Chromosome_\", i, \"_nonSmoothed.1kb.pdf\")),     p1,     dpi = 500,      height = 9,     units = \"in\",      width = 16   )    # ggsave(p2,   #   filename = file.path(plot_out_prefix, paste0(\"Chromosome_\", i, \"_Lungs15days_nonSmoothed.1kb.pdf\")), dpi = 500, height = 9,   #   units = \"in\", width = 16   # )   #    # ggsave(p3,   #   filename = file.path(plot_out_prefix, paste0(\"Chromosome_\", i, \"_Brains15days_nonSmoothed.1kb.pdf\")), dpi = 500, height = 9,   #   units = \"in\", width = 16 } #### IR-1 PLOTS i <- 2 rm(temp) rm(PoolsCombine) rm(newMinsAndMax) temp <- rbind(   samplesNonSmooth[[\"L\"]][samplesNonSmooth[[\"L\"]]$CHROM == paste0(\"chr\", i), ],   samplesNonSmooth[[\"Y\"]][samplesNonSmooth[[\"Y\"]]$CHROM == paste0(\"chr\", i), ] ) temp$combine <- paste(temp$Significance, temp$Condition, sep = \"_\") levels(temp$combine) <- c(\"FALSE_Lungs\", \"FALSE_YPD\", \"TRUE_Lungs\", \"TRUE_YPD\") eachPool <- c(\"P1sig\", \"P2sig\", \"P3sig\", \"P4sig\", \"P5sig\") PoolsCombine <- lapply(eachPool, FUN = function(x) {   z <- paste(temp[, grep(x, colnames(temp))], temp$Condition, sep = \"_\")   return(z) })  newMinsAndMax <- MaxAndMins %>% filter(CHROM == paste0(\"chr\", i))  p1 <- ggplot(data = temp, aes(x = temp$binMiddle / 1000, y = temp$All_pools)) +   geom_rect(xmin = 283.162, xmax = 468.855, ymin = -1.1, ymax = 1.1, color = \"yellow\", fill = \"yellow\", alpha = 0.01) +   geom_line(data = newMinsAndMax, aes(x = POS / 1000, y = Min_SNPchange)) +   geom_line(data = newMinsAndMax, aes(x = POS / 1000, y = Max_SNPchange)) +   geom_pointrange(     data = temp,     aes(       ymin = pool_min,       ymax = pool_max,       x = binMiddle / 1000,       y = All_pools,       size = combine,       colour = combine,       alpha = combine,       order = \"combine\"     )   ) +   scale_size_manual(     name = \"Condition and significance\",     labels = c(\"Lung, not significant\", \"YPD, not significant\", \"Lung, significant\", \"YPD, significant\"),     values = c(0.1, 0.1, 0.4, 0.4), drop = F   ) +   scale_colour_manual(     name = \"Condition and significance\",     labels = c(\"Lung, not significant\", \"YPD, not significant\", \"Lung, significant\", \"YPD, significant\"),     values = c(       \"FALSE_Lungs\" = \"red\", \"FALSE_YPD\" = \"black\",       \"TRUE_Lungs\" = \"red\", \"TRUE_YPD\" = \"black\"     ), drop = F   ) +   scale_alpha_manual(     name = \"Condition and significance\",     labels = c(\"Lung, not significant\", \"YPD, not significant\", \"Lung, significant\", \"YPD, significant\"),     values = c(0.95, 0.95, 1, 1), drop = F   ) +   labs(title = paste0(\"Chromosome \", i), x = paste0(\"Position on chromosome \", i, \" (kb)\"), y = \"Change in allele frequency (BSA)\", color = \"Pool\") +   geom_hline(yintercept = c(0), color = \"black\", size = 0.6) +   scale_x_continuous(breaks = seq(from = 0, to = 2400, by = 300), limits = c(0, 1650), expand = c(0, 0)) +   scale_y_continuous(breaks = c(-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1), limits = c(-1.1, 1.1), expand = c(0, 0)) +   theme( # legend.title = element_text(size=18),     legend.position = \"none\",     plot.title = element_blank(),     legend.text = element_text(size = 16),     axis.title = element_text(size = 32),     axis.title.y = element_text(margin = margin(r = 25, t = 0, l = 5, b = 0)),     axis.title.x = element_text(margin = margin(r = 0, t = 25, l = 0, b = 5)),     axis.text = element_text(size = 28),     axis.ticks.length = unit(0.3, \"cm\"),     panel.grid.major = element_blank(),     panel.grid.minor = element_blank(),     panel.background = element_rect(fill = \"white\", colour = \"black\", size = 1),     text = element_text(family = \"sans\")   )  ggsave(filename = file.path(plot_out_prefix, paste0(\"IR-1_5kb-NONsmoothed.pdf\")),         p1,        dpi = 500,         height = 9,        units = \"in\",         width = 16 ) ### IR-1 Zoom PLOT PoolsZoom <- list() allPoolsZoom <- list() for (i in names(pools_filteredBSA)) {   PoolsZoom[[i]] <- pools_filteredBSA[[i]] %>%     dplyr::filter((CHROM == \"chr2\" & POS > 249500 & POS < 500500)) %>%     mutate(Significance = F)   for (j in 1:nrow(PoolsZoom[[i]])) {     if (PoolsZoom[[i]]$qvalue[j] < 0.1) {       PoolsZoom[[i]]$Significance[j] <- TRUE     }   } } for (i in names(allPoolsInOne_filteredBSA)) {   allPoolsZoom[[i]] <- allPoolsInOne_filteredBSA[[i]] %>%     dplyr::filter((CHROM == \"chr2\" & POS > 249500 & POS < 500500)) %>%     mutate(Significance = F)   for (j in 1:nrow(allPoolsZoom[[i]])) {     if (allPoolsZoom[[i]]$qvalue[j] < 0.1) {       allPoolsZoom[[i]]$Significance[j] <- TRUE     }   } }   names(PoolsZoom) <- c(\"P1.Y\", \"P1.L\", \"P2.Y\", \"P2.L\", \"P3.Y\", \"P3.L\", \"P4.Y\", \"P4.L\", \"P5.Y\", \"P5.L\", \"Pall.Y\", \"Pall.L\") names(allPoolsZoom) <- c(\"Y\", \"L\") condition <- c(\"YPD\", \"Inoculum\") chosenSize <- 500 Pools <- list() allPools <- list()  ptm <- proc.time() no_cores <- detectCores() - 3 # Use all cores but one, so the computer can do other things while it runs cl <- makeCluster(no_cores) clusterExport(cl, c(\"PoolsZoom\", \"allPoolsZoom\", \"binner2\", \"Pools\", \"allPools\", \"chosenSize\")) # Calling the binner2 in parallel Pools <- parLapply(   cl, names(PoolsZoom),   function(x) {     binner2(PoolsZoom[[grep(x, names(PoolsZoom))]], bin.size = chosenSize)   } ) allPools <- parLapply(   cl, names(allPoolsZoom),   function(x) {     binner2(allPoolsZoom[[grep(x, names(allPoolsZoom))]], bin.size = chosenSize)   } ) stopCluster(cl) # Closing the parallel cluster proc.time() - ptm  names(Pools) <- c(\"P1.Y\", \"P1.L\", \"P2.Y\", \"P2.L\", \"P3.Y\", \"P3.L\", \"P4.Y\", \"P4.L\", \"P5.Y\", \"P5.L\", \"Pall.Y\", \"Pall.L\") names(allPools) <- c(\"Y\", \"L\") condition <- c(\"YPD\", \"Inoculum\") samples <- lapply(c(1, 2), FUN = function(x) {   data.frame(     CHROM = allPools[[names(allPools)[x]]]$CHROM,     binFloor = allPools[[names(allPools)[x]]]$binFloor,     binCeiling = allPools[[names(allPools)[x]]]$binCeiling,     binMiddle = allPools[[names(allPools)[x]]]$binMiddle,     Bin_size = allPools[[names(allPools)[x]]]$Bin_size,     P1 = Pools[[paste0(\"P1.\", names(allPools)[x])]]$smoothedDeltaSNP,     P1sig = Pools[[paste0(\"P1.\", names(allPools)[x])]]$Significance,     P2 = Pools[[paste0(\"P2.\", names(allPools)[x])]]$smoothedDeltaSNP,     P2sig = Pools[[paste0(\"P2.\", names(allPools)[x])]]$Significance,     P3 = Pools[[paste0(\"P3.\", names(allPools)[x])]]$smoothedDeltaSNP,     P3sig = Pools[[paste0(\"P3.\", names(allPools)[x])]]$Significance,     P4 = Pools[[paste0(\"P4.\", names(allPools)[x])]]$smoothedDeltaSNP,     P4sig = Pools[[paste0(\"P4.\", names(allPools)[x])]]$Significance,     P5 = Pools[[paste0(\"P5.\", names(allPools)[x])]]$smoothedDeltaSNP,     P5sig = Pools[[paste0(\"P5.\", names(allPools)[x])]]$Significance,     Pall = Pools[[paste0(\"Pall.\", names(allPools)[x])]]$smoothedDeltaSNP,     Pallsig = Pools[[paste0(\"Pall.\", names(allPools)[x])]]$Significance,     All_pools = allPools[[names(allPools)[x]]]$smoothedDeltaSNP,     mut_perBin = allPools[[names(allPools)[x]]]$mut_perBin,     Significance = allPools[[names(allPools)[x]]]$Significance   ) %>%     mutate(pool_max = pmax(P1, P2, P3, P4, P5, na.rm = T), pool_min = pmin(P1, P2, P3, P4, P5, na.rm = T), Condition = rep(condition[x], nrow(allPools[[names(allPools)[x]]]))) })  names(samples) <- names(allPools) samplesZoom <- samples  i <- 2 rm(temp) rm(PoolsCombine) temp <- rbind(   samplesZoom[[\"L\"]][samplesZoom[[\"L\"]]$CHROM == paste0(\"chr\", i), ],   samplesZoom[[\"Y\"]][samplesZoom[[\"Y\"]]$CHROM == paste0(\"chr\", i), ] ) temp$combine <- paste(temp$Significance, temp$Condition, sep = \"_\") levels(temp$combine) <- c(\"FALSE_Inoculum\", \"FALSE_YPD\", \"TRUE_Inoculum\", \"TRUE_YPD\") eachPool <- c(\"P1sig\", \"P2sig\", \"P3sig\", \"P4sig\", \"P5sig\", \"Pallsig\") PoolsCombine <- lapply(eachPool, FUN = function(x) {   z <- paste(temp[, grep(x, colnames(temp))], temp$Condition, sep = \"_\")   return(z) }) p1 <- ggplot(data = temp, aes(x = temp$binMiddle / 1000, y = temp$All_pools)) +   geom_pointrange(     data = temp,     aes(       ymin = pool_min,       ymax = pool_max,       x = binMiddle / 1000,       y = All_pools,       size = combine,       colour = combine,       alpha = combine,       order = \"combine\"     )   ) +   scale_size_manual(     name = \"Condition and significance\",     labels = c(\"Lung, not significant\", \"YPD, not significant\", \"Lung, significant\", \"YPD, significant\"),     values = c(0.1, 0.1, 0.4, 0.4), drop = F   ) +   scale_colour_manual(     name = \"Condition and significance\",     labels = c(\"Lung, not significant\", \"YPD, not significant\", \"Lung, significant\", \"YPD, significant\"),     values = c(       \"FALSE_Inoculum\" = \"red\", \"FALSE_YPD\" = \"black\",       \"TRUE_Inoculum\" = \"red\", \"TRUE_YPD\" = \"black\"     ), drop = F   ) +   scale_alpha_manual(     name = \"Condition and significance\",     labels = c(\"Lung, not significant\", \"YPD, not significant\", \"Lung, significant\", \"YPD, significant\"),     values = c(0.95, 0.95, 1, 1), drop = F   ) +   geom_vline(xintercept = c(283.162, 468.855), color = \"black\", size = 0.3, linetype = \"dashed\") +   # geom_vline(xintercept = 0, size=0.3)+   labs(title = paste0(\"Chromosome \", i), x = paste0(\"Position on chromosome \", i, \" (kb)\"), y = \"Change in allele frequency (BSA)\", color = \"Pool\") +   geom_hline(yintercept = c(0), color = \"black\", size = 0.6) +   scale_x_continuous(breaks = seq(from = 0, to = 2400, by = 50), limits = c(250, 500), expand = c(0, 0)) +   scale_y_continuous(breaks = c(-0.5, -0.25, 0, 0.25, 0.5), limits = c(-0.60, 0.60), expand = c(0, 0)) +   theme( # legend.title = element_text(size=18),     legend.position = \"none\",     plot.title = element_blank(),     legend.text = element_text(size = 16),     axis.title = element_text(size = 32),     axis.title.y = element_text(margin = margin(r = 25, t = 0, l = 5, b = 0)),     axis.title.x = element_text(margin = margin(r = 0, t = 25, l = 0, b = 5)),     axis.text = element_text(size = 28),     axis.ticks.length = unit(0.3, \"cm\"),     panel.grid.major = element_blank(),     panel.grid.minor = element_blank(),     panel.background = element_rect(fill = \"white\", colour = \"black\", size = 1),     text = element_text(family = \"sans\")   )   ggsave(p1, filename = file.path(plot_out_prefix, paste0(\"IR-1zoom.pdf\")), dpi = 500, height = 9, units = \"in\", width = 16)"},{"path":"https://cmatkhan.github.io/BSA/articles/processing_instructions.html","id":"alignment","dir":"Articles","previous_headings":"","what":"Alignment","title":"processing_instructions","text":"first step align reads. normally name samples according , ’s represented fastq file. first step createan ID file containing name sample file full address, separated space.can change sep script. considering changing sep toa comma, easily exported Excel. haven’t done . name sample fastq file allows use simple script tomake ID file. BSA2 samples, look excel sheet DanielSeqDatabase.xlsx find files interest . just :","code":"cp /lts/mblab/personal/daniel.agustinho/raw/BSA2/*.fastq.gz /scratch/$USER/bsa2_fastq  cd /scratch/$USER/bsa2   /scratch/mblab/daniel.agustinho/tools/IDmaker.sh ID"},{"path":"https://cmatkhan.github.io/BSA/articles/processing_instructions.html","id":"idmaker-sh","dir":"Articles","previous_headings":"Alignment","what":"IDmaker.sh","title":"processing_instructions","text":"create file called ID containing name sample. Copy fastq.gz files analysis folder scratch. Create useful folders beused scripts. Call alignments NGM (can use another). Observe thatthe script already “KN99 reference” ’s older. Feel free adjust inyour copied version script.","code":"[chasem@login ~]$ cat /scratch/mblab/daniel.agustinho/tools/IDmaker.sh  output=$1  ls *R1_001.fastq.gz |awk -F '_' '{print $3}' > temp1.txt ls -d \"$PWD\"/*R1_001.fastq.gz > temp2.txt paste -d \" \" temp1.txt temp2.txt > $output rm temp1.txt temp2.txt mkdir -p bams freebayes/indvVCF   sbatch raw log SVssbatch --array=1-$(wc -l < ID)  /scratch/mblab/daniel.agustinho/tools/ngm.job ID bams KN99"},{"path":"https://cmatkhan.github.io/BSA/articles/processing_instructions.html","id":"ngm-job","dir":"Articles","previous_headings":"Alignment","what":"ngm.job","title":"processing_instructions","text":"create alignment files (bam bai) sample bams folder. Itwill also call CNVnator one , generating tables CNV VCFs sample names SVs/indvCNVs folder. call Delly SVs, theSVs/delly folder. recently observed Delly sucks. recommend using Manta instead. also call individual VCFs using freebayes annotating withSNPEff freebayes/indvVCF folder. problems errors, check log/ folder","code":"[chasem@login ~]$ cat /scratch/mblab/daniel.agustinho/tools/IDmaker.sh output=$1  ls *R1_001.fastq.gz |awk -F '_' '{print $3}' > temp1.txt ls -d \"$PWD\"/*R1_001.fastq.gz > temp2.txt paste -d \" \" temp1.txt temp2.txt > $output rm temp1.txt temp2.txt    [chasem@login ~]$ cat /scratch/mblab/daniel.agustinho/tools/ngm.job #!/usr/bin/env bash #SBATCH -o log/ngm-%a-out #SBATCH -e log/ngm-%a-err #SBATCH --mem=12000  ml lumpy-sv ml bamaddrg ml nextgenmap/0.5.3  ml yaha/0.1.83 ml picard-tools/2.10.0 ml freebayes/1.1.0 ml vcftools/0.1.14 ml snpeff/4.1 ml tabix  # $1 is the list of IDs and sequencing tags, $2 is the directory with the FASTQ files, $3 is the directory to output to  # No seq Folder on this version  file=$1 outputDir=$2 genome=$3 line=$SLURM_ARRAY_TASK_ID strainLine=`sed \"${line}q;d\" ${file}` Sample=`echo ${strainLine} | cut -d \" \" -f 2` name=`echo ${strainLine} | cut -d \" \" -f 1`  # Defining the preset genomes: KN99, H99 and mouse if [ $genome =  \"H99\" ];then  reference=\"/scratch/mblab/daniel.agustinho/references/crNeoH99.fasta\"  echo \"You chose the H99 genome as a reference genome: ${reference}\" elif [ $genome = \"KN99\" ];then  reference=\"/scratch/mblab/daniel.agustinho/references/crNeoKN99.fasta\"  echo \"You chose the KN99 genome as a reference genome: ${reference}\" elif [ $genome = \"mouse\" ];then  reference=\"/scratch/mblab/daniel.agustinho/references/mm10.fa\"  echo \"You chose the MM10 mouse genome as a reference genome: ${reference}\" else     echo \"You chose a genome different from the preset ones (KN99, H99 or Mouse MM10).\"     reference=$genome     echo $reference fi  mkdir -p ${outputDir}/splitReads mkdir -p freebayes/indvVCF/snpEff/results/  file2=${Sample/R1_001.fastq/R2_001.fastq} if [ -f $file2 ]; then     #This aligns to reference and converts to ngm.bam, plus indexes the bam ngm -X 100000000 -t 5 --rg-id ${name} --rg-sm ${name} -1 ${Sample} -2 ${file2} -r $reference |samtools view -bh - |samtools sort - |samtools rmdup - ${outputDir}/${name}.ngm.bam else #This aligns to reference and converts to ngm.bam, plus indexes the bam ngm -X 100000000 -t 5 --rg-id ${name} --rg-sm ${name} -q ${Sample} -r $reference |samtools view -bh - |samtools sort - |samtools rmdup - ${outputDir}/${name}.ngm.bam fi  samtools index ${outputDir}/${name}.ngm.bam  #This gets the split reads from the bam into a fastq for YAHA samtools view -h ${outputDir}/${name}.ngm.bam | /opt/apps/lumpy-sv/lumpy-sv-0.2.13/scripts/split_unmapped_to_fasta.pl -b 20 >${outputDir}/splitReads/${name}.split.fq  yaha -t 15 -x ${reference/fasta/X11_01_02000S} -q ${outputDir}/splitReads/${name}.split.fq -osh stdout  -M 15 -H 2000 -L 11 |samtools view -Sbh - |samtools sort - > ${outputDir}/splitReads/${name}.split.ngm.bam  #java -jar /opt/apps/picard-tools/2.10.0/picard.jar MergeSamFiles I=${outputDir}/${name}.ngm.bam I=${outputDir}/splitReads/${name}.split.ngm.bam O=${outputDir}/${name}.merged.ngm.bam USE_THREADING=true AS=true MERGE_SEQUENCE_DICTIONARIES=true  java -jar /scratch/mblab/daniel.agustinho/tools/repo/PicardTools/picard.jar MergeSamFiles I=${outputDir}/${name}.ngm.bam I=${outputDir}/splitReads/${name}.split.ngm.bam O=${outputDir}/${name}.merged.ngm.bam USE_THREADING=true AS=true MERGE_SEQUENCE_DICTIONARIES=true  bamaddrg -s ${name} -b ${outputDir}/${name}.merged.ngm.bam > ${outputDir}/${name}.merged.tagged.ngm.bam #bamaddrg -s ${name} -b ${outputDir}/${name}.ngm.bam > ${outputDir}/${name}.tagged.ngm.bam samtools index ${outputDir}/${name}.merged.tagged.ngm.bam  rm ${outputDir}/${name}.ngm.bam.bai rm ${outputDir}/${name}.merged.ngm.bam rm ${outputDir}/${name}.ngm.bam  sbatch /scratch/mblab/daniel.agustinho/tools/cnvnator.job $file bams/ SVs/ $line $genome sbatch /scratch/mblab/daniel.agustinho/tools/delly.sh $file bams/ SVs/ $reference $line   freebayes -F 0.75 -! 5 -p 1 --min-mapping-quality 30 -f $reference ${outputDir}/${name}.merged.tagged.ngm.bam > freebayes/indvVCF/$name.vcf   java -jar /home/daniel.agustinho/miniconda3/pkgs/snpeff-4.3.1t-0/share/snpeff-4.3.1t-0/snpEff.jar -c /home/daniel.agustinho/miniconda3/pkgs/snpeff-4.3.1t-0/share/snpeff-4.3.1t-0/snpEff.config -s snpEff/results/${name}.html -v ASM221672v1 freebayes/indvVCF/$name.vcf > freebayes/indvVCF/$name.ann.vcf bgzip freebayes/indvVCF/$name.ann.vcf tabix freebayes/indvVCF/$name.ann.vcf.gz"},{"path":"https://cmatkhan.github.io/BSA/articles/processing_instructions.html","id":"variant-calling-for-bsa-analysis","dir":"Articles","previous_headings":"","what":"Variant calling for BSA analysis","title":"processing_instructions","text":"finished, call freebayes population VCF. First, create list bams, use freebayes call","code":"ls -d \"$PWD\"/bams/*.merged.tagged.ngm.bam >> bamList  /scratch/mblab/daniel.agustinho/tools/sbatch_maker/freebayes.bamList.sbatch \\   KN99 \\   BSA2 \\   bamList"},{"path":"https://cmatkhan.github.io/BSA/articles/processing_instructions.html","id":"freebayes-bamlist-sbatch","dir":"Articles","previous_headings":"Variant calling for BSA analysis","what":"freebayes.bamList.sbatch","title":"processing_instructions","text":"","code":"[chasem@login ~]$ cat /scratch/mblab/daniel.agustinho/tools/sbatch_maker/freebayes.bamList.sbatch #!/usr/bin/env bash  #SBATCH -o freebayes-out #SBATCH -e freebayes-err #SBATCH --mem=50000 #SBATCH --mail-type=END,FAIL #SBATCH --array 1-17%17 ml freebayes/1.1.0 ml vcftools/0.1.14  genome=$1 # Either H99 or KN99 name=$2 #This is the name of the clinical strain used for basis of the comparison bamList=$3  if [ $genome =  \"H99\" ];then  reference=\"/scratch/mblab/daniel.agustinho/references/crNeoH99.fasta\" elif [ $genome = \"KN99\" ];then  reference=\"/scratch/mblab/daniel.agustinho/references/crNeoKN99.fasta\" else     echo \"Valid genomes are H99 or KN99. Review your 4th  argument.\"     exit fi  /scratch/mblab/daniel.agustinho/tools/freebayes.bam_list.job $reference $name $bamList ${SLURM_ARRAY_TASK_ID}"},{"path":"https://cmatkhan.github.io/BSA/articles/processing_instructions.html","id":"freebayes-bam_list-job","dir":"Articles","previous_headings":"Variant calling for BSA analysis","what":"freebayes.bam_list.job","title":"processing_instructions","text":"create one VCF file per chromosome freebayes folder. Check ransmoothly (samples may require memmory), , merge allof together using VCFmerger.sh script. script merges one bigVCF deletes VCF individual chromosomes, sure previous stepran smoothly. also calls next script, makes tables VCF file canbe easily read R script analysis.","code":"[chasem@login ~]$ cat /scratch/mblab/daniel.agustinho/tools/freebayes.bam_list.job #!/usr/bin/env bash #SBATCH --mem=60G  ml freebayes/1.1.0 ml vcftools/0.1.14  reference=$1 name=$2 bamList=$3 region=$4 chrom=$(gawk '{print $1}' $reference.fai |sed \"${region}q;d\") parentVCF=$5 echo $parentVCF mkdir -p freebayes  if [ -z $5  ];then  freebayes -! 5 -F 0.75 -p 1 --min-mapping-quality 30 -r $chrom -f $reference --bam-list $bamList > freebayes/freebayes.$name.$chrom.vcf else  freebayes -l -@ $parentVCF -! 5 -F 0.75 -p 1 --min-mapping-quality 30 -r $chrom -f $reference --bam-list $bamList > freebayes/freebayes.$name.$chrom.vcf fi scratch/mblab/daniel.agustinho/tools/VCFmerger.sh BSA2"},{"path":"https://cmatkhan.github.io/BSA/articles/processing_instructions.html","id":"vcfmerger-sh","dir":"Articles","previous_headings":"Variant calling for BSA analysis","what":"VCFmerger.sh","title":"processing_instructions","text":"","code":"[chasem@login ~]$ cat /scratch/mblab/daniel.agustinho/tools/VCFmerger.sh  #!/usr/bin/env bash #SBATCH -o log/VCFmerger_%j-out #SBATCH -e log/VCFmerger_%j-err #SBATCH --mem=50G  ml vcftools/0.1.14 ml R  name=$1  mkdir -p freebayes/VCFtables  grep '#' freebayes/freebayes.${name}.chr1.vcf>freebayes/$name.vcf for i in chr{1..14} chrM NAT G418 do cat freebayes/freebayes.${name}.$i.vcf | grep -v '^#' >> freebayes/$name.vcf  done  vcftools --vcf freebayes/${name}.vcf --out freebayes/${name}.dusted.vcf --exclude-bed /scratch/mblab/daniel.agustinho/references/dustmasked.KN99.bed --recode --recode-INFO-all mv freebayes/$name.dusted.vcf.recode.vcf freebayes/$name.clean.vcf  rm freebayes/freebayes.${name}.chr*.vcf rm freebayes/$name.dusted.vcf.recode.vcf rm freebayes/${name}.vcf  # Starting the sorting. sed -e \"77s/#//\" freebayes/$name.clean.vcf > freebayes/$name.clean.vcf.test Rscript /scratch/mblab/daniel.agustinho/tools/vcfSorter.R freebayes/$name.clean.vcf.test  grep '^##' freebayes/$name.clean.vcf  > freebayes/$name.sorted.vcf # ${file/clean.vcf/sorted.vcf} cat data.vcf >> freebayes/$name.sorted.vcf  sed -e \"77s/CHROM/#CHROM/\" freebayes/$name.sorted.vcf > freebayes/$name.IGV.vcf  rm data.vcf rm freebayes/$name.clean.vcf.test freebayes/$name.clean.vcf freebayes/$name.sorted.vcf  /scratch/mblab/daniel.agustinho/tools/VCF_tabler.sh freebayes/$name.IGV.vcf freebayes/VCFtables rm freebayes/VCFtables/.txt /scratch/mblab/daniel.agustinho/tools/strainAnalyzer.sh ./popAnalyzer.sh"},{"path":"https://cmatkhan.github.io/BSA/articles/processing_instructions.html","id":"statistical-analysis","dir":"Articles","previous_headings":"","what":"Statistical analysis","title":"processing_instructions","text":"NOTE scripts incorporated R/ directory vignettes. Daniel: create BSA2.IGV.vcf file freebayes folder. can look IGV etc. also create folder freebayes/VCFtables contains one table (txtfile) sample called used input freebayes. Transfer toyour personal laptop analyse using R script: BSA2Analysis.R Adjust script get input proper folder (txt files copied server ). generate BSA2.RData file containing analysis. can use second R script make plots. BSA2plots.R","code":""},{"path":"https://cmatkhan.github.io/BSA/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"First Last. Author, maintainer.","code":""},{"path":"https://cmatkhan.github.io/BSA/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"cmatKhan (2022). BSA -temp1. doi:10.18129/B9.bioc.BSA, https://github.com/cmatKhan/BSA/BSA - R package version 0.0.0.9000, http://www.bioconductor.org/packages/BSA. cmatKhan (2022). “BSA - temp2.” bioRxiv. doi:10.1101/TODO, https://www.biorxiv.org/content/10.1101/TODO.","code":"@Manual{,   title = {BSA -temp1},   author = {{cmatKhan}},   year = {2022},   url = {http://www.bioconductor.org/packages/BSA},   note = {https://github.com/cmatKhan/BSA/BSA - R package version 0.0.0.9000},   doi = {10.18129/B9.bioc.BSA}, } @Article{,   title = {BSA - temp2},   author = {{cmatKhan}},   year = {2022},   journal = {bioRxiv},   doi = {10.1101/TODO},   url = {https://www.biorxiv.org/content/10.1101/TODO}, }"},{"path":"https://cmatkhan.github.io/BSA/index.html","id":"bsa","dir":"","previous_headings":"","what":"What the Package Does (One Line, Title Case)","title":"What the Package Does (One Line, Title Case)","text":"goal BSA reimplement Daniel’s BSA2 code package structure may installed using R install. may also form seed either software workflow package bioconductor.","code":""},{"path":"https://cmatkhan.github.io/BSA/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"What the Package Does (One Line, Title Case)","text":"","code":"install.packages(\"remotes\")  remotes::install_github(\"cmatKhan/BSA\")"},{"path":[]},{"path":"https://cmatkhan.github.io/BSA/index.html","id":"installation-instructions","dir":"","previous_headings":"","what":"Installation instructions","title":"What the Package Does (One Line, Title Case)","text":"Get latest stable R release CRAN. install BSA Bioconductor using following code: development version GitHub :","code":"if (!requireNamespace(\"BiocManager\", quietly = TRUE)) {     install.packages(\"BiocManager\") }  BiocManager::install(\"BSA\") BiocManager::install(\"cmatKhan/BSA\")"},{"path":"https://cmatkhan.github.io/BSA/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"What the Package Does (One Line, Title Case)","text":"basic example shows solve common problem: special using README.Rmd instead just README.md? can include R chunks like : ’ll still need render README.Rmd regularly, keep README.md --date. can also embed plots, example:  case, don’t forget commit push resulting figure files, display GitHub!","code":"library(\"BSA\") ## basic example code summary(cars) #>      speed           dist        #>  Min.   : 4.0   Min.   :  2.00   #>  1st Qu.:12.0   1st Qu.: 26.00   #>  Median :15.0   Median : 36.00   #>  Mean   :15.4   Mean   : 42.98   #>  3rd Qu.:19.0   3rd Qu.: 56.00   #>  Max.   :25.0   Max.   :120.00"},{"path":"https://cmatkhan.github.io/BSA/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"What the Package Does (One Line, Title Case)","text":"citation output using citation('BSA') R. Please run check updates cite BSA. Please note BSA made possible thanks many R bioinformatics software authors, cited either vignettes /paper(s) describing package.","code":"print(citation('BSA'), bibtex = TRUE) #>  #> To cite package 'BSA' in publications use: #>  #>   cmatKhan (2022). _BSA -temp1_. doi:10.18129/B9.bioc.BSA #>   <https://doi.org/10.18129/B9.bioc.BSA>, #>   https://github.com/cmatKhan/BSA/BSA - R package version 0.0.0.9000, #>   <http://www.bioconductor.org/packages/BSA>. #>  #> A BibTeX entry for LaTeX users is #>  #>   @Manual{, #>     title = {BSA -temp1}, #>     author = {{cmatKhan}}, #>     year = {2022}, #>     url = {http://www.bioconductor.org/packages/BSA}, #>     note = {https://github.com/cmatKhan/BSA/BSA - R package version 0.0.0.9000}, #>     doi = {10.18129/B9.bioc.BSA}, #>   } #>  #>   cmatKhan (2022). \"BSA - temp2.\" _bioRxiv_. doi:10.1101/TODO #>   <https://doi.org/10.1101/TODO>, #>   <https://www.biorxiv.org/content/10.1101/TODO>. #>  #> A BibTeX entry for LaTeX users is #>  #>   @Article{, #>     title = {BSA - temp2}, #>     author = {{cmatKhan}}, #>     year = {2022}, #>     journal = {bioRxiv}, #>     doi = {10.1101/TODO}, #>     url = {https://www.biorxiv.org/content/10.1101/TODO}, #>   }"},{"path":"https://cmatkhan.github.io/BSA/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"What the Package Does (One Line, Title Case)","text":"Please note BSA project released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"https://cmatkhan.github.io/BSA/index.html","id":"development-tools","dir":"","previous_headings":"","what":"Development tools","title":"What the Package Does (One Line, Title Case)","text":"Continuous code testing possible thanks GitHub actions usethis, remotes, rcmdcheck customized use Bioconductor’s docker containers BiocCheck. Code coverage assessment possible thanks codecov covr. documentation website automatically updated thanks pkgdown. code styled automatically thanks styler. documentation formatted thanks devtools roxygen2. details, check dev directory. package developed using biocthis.","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/analyzer.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze stuff — analyzer","title":"Analyze stuff — analyzer","text":"Now list comparisons replicates (object: \"replicates\"), pools (object: \"pools\") samples together (allPoolsInOneComparison) can begin filtering samples. First idea filter following params","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/analyzer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze stuff — analyzer","text":"","code":"analyzer(   SNPcomparison,   minDepthPercentile = 0.1,   maxDepthPercentile = 0.9,   windowSize = 25000,   bulkSize = 20,   outlierFilt = \"deltaSNP\",   filter_chr_list = NULL )"},{"path":"https://cmatkhan.github.io/BSA/reference/analyzer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze stuff — analyzer","text":"minDepthPercentile sample depth - Half min depth, Default: 0.9 maxDepthPercentile depth: higher fifth percentile > quantile(sum depth bulks, na.rm = T, probs = 0.95)), Default: 0.1 windowSize PARAM_DESCRIPTION, Default: 25000 bulkSize PARAM_DESCRIPTION, Default: 20 outlierFilt PARAM_DESCRIPTION, Default: 'deltaSNP' filter_chr_list PARAM_DESCRIPTION, Default: NULL SNPcomparison: lower tenth percentile > quantile(sum depth bulks, na.rm = T, probs = 0.1))","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/analyzer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze stuff — analyzer","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/analyzer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Analyze stuff — analyzer","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binDistPlotter.html","id":null,"dir":"Reference","previous_headings":"","what":"plot bin dist — binDistPlotter","title":"plot bin dist — binDistPlotter","text":"function makes distribution plots bins, using output table binner()palette() function input.","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binDistPlotter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plot bin dist — binDistPlotter","text":"","code":"binDistPlotter(   inputBinTable,   windowsSizes = c(5000, 10000, 20000, 50000),   fileName )"},{"path":"https://cmatkhan.github.io/BSA/reference/binDistPlotter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plot bin dist — binDistPlotter","text":"inputBinTable PARAM_DESCRIPTION windowsSizes PARAM_DESCRIPTION, Default: c(5000, 10000, 20000, 50000) fileName PARAM_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binDistPlotter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"plot bin dist — binDistPlotter","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binDistPlotter.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"plot bin dist — binDistPlotter","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner.html","id":null,"dir":"Reference","previous_headings":"","what":"Binner -- the first — binner","title":"Binner -- the first — binner","text":"function make new table different bin sizes average ∆% observed positions inside bin. can play bin size, uses output percenter input.","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binner -- the first — binner","text":"","code":"binner(input, bin.size = 10000)"},{"path":"https://cmatkhan.github.io/BSA/reference/binner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binner -- the first — binner","text":"input PARAM_DESCRIPTION bin.size size window used smoothing. Default: 10000","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binner -- the first — binner","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binner -- the first — binner","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner2.html","id":null,"dir":"Reference","previous_headings":"","what":"Binner the second — binner2","title":"Binner the second — binner2","text":"diff binner? function make new table different bin sizes average ∆% observed positions inside bin. can play bin size, uses output percenter input.","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binner the second — binner2","text":"","code":"binner2(input, bin.size = 10000)"},{"path":"https://cmatkhan.github.io/BSA/reference/binner2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binner the second — binner2","text":"input PARAM_DESCRIPTION bin.size PARAM_DESCRIPTION, Default: 10000","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binner the second — binner2","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Binner the second — binner2","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner2.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Binner the second — binner2","text":"bin considered significant half snps bin significant","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner3.html","id":null,"dir":"Reference","previous_headings":"","what":"Another binner — binner3","title":"Another binner — binner3","text":"diff binner?","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner3.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Another binner — binner3","text":"","code":"binner3(input, bin.size = 10000)"},{"path":"https://cmatkhan.github.io/BSA/reference/binner3.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Another binner — binner3","text":"input PARAM_DESCRIPTION bin.size PARAM_DESCRIPTION, Default: 10000","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner3.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Another binner — binner3","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/binner3.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Another binner — binner3","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/getG.html","id":null,"dir":"Reference","previous_headings":"","what":"get G prime stat — getG","title":"get G prime stat — getG","text":"function used runGprimeAnalysis calculate G statisic G defined equation: $$G = 2*\\sum_{=1}^{4}n_{}*ln\\frac{obs(n_i)}{exp(n_i)}$$ SNP, \\(n_i\\) = 1 4 corresponds reference alternate allele depths bulk, described following table: ...\\(obs(n_i)\\) observed allele depths described data frame. Method 1 calculates G statistic using expected values assuming read depth equal alleles bulks: $$exp(n_1) = ((n_1 + n_2)*(n_1 + n_3))/(n_1 + n_2 + n_3 + n_4)$$ $$exp(n_2) = ((n_2 + n_1)*(n_2 + n_4))/(n_1 + n_2 + n_3 + n_4)$$ etc...","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/getG.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get G prime stat — getG","text":"","code":"getG(LowRef, HighRef, LowAlt, HighAlt)"},{"path":"https://cmatkhan.github.io/BSA/reference/getG.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"get G prime stat — getG","text":"LowRef vector reference allele depth low bulk HighRef vector reference allele depth high bulk LowAlt vector alternate allele depth low bulk HighAlt vector alternate allele depth high bulk","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/getG.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"get G prime stat — getG","text":"vector G statistic values length ","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/getG.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"get G prime stat — getG","text":"DETAILS","code":""},{"path":[]},{"path":"https://cmatkhan.github.io/BSA/reference/getPvals.html","id":null,"dir":"Reference","previous_headings":"","what":"get g prime pvals — getPvals","title":"get g prime pvals — getPvals","text":"function used runGprimeAnalysis estimate p-values weighted G' statistic based non-parametric estimation method described Magwene et al. 2011. Breifly, using natural log Gprime median absolute deviation (MAD) calculated. Gprime set trimmed exclude outlier regions (.e. QTL) based Hampel's rule. alternate method filtering QTL proposed using absolute delta SNP indeces greater set threshold filter potential QTL. estimation mode trimmed set calculated using mlv function package modeest. Finally, mean variance set estimated using median mode p-values estimated log normal distribution.","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/getPvals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get g prime pvals — getPvals","text":"","code":"getPvals(   Gprime,   deltaSNP = NULL,   outlierFilter = c(\"deltaSNP\", \"Hampel\"),   filterThreshold )"},{"path":"https://cmatkhan.github.io/BSA/reference/getPvals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"get g prime pvals — getPvals","text":"Gprime vector G prime values (tricube weighted G statistics) deltaSNP vector delta SNP values use QTL region filtering outlierFilter one either \"deltaSNP\" \"Hampel\". Method filtering outlier (ie QTL) regions p-value estimation filterThreshold absolute delta SNP index use filter putative QTL","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/getPvals.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"get g prime pvals — getPvals","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/getPvals.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"get g prime pvals — getPvals","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/percenter.html","id":null,"dir":"Reference","previous_headings":"","what":"a percenter function — percenter","title":"a percenter function — percenter","text":"function make new table differences percentages (YPD - inoculum, Lung - inoculum) sample using tabler output list input. sampleColumn column number shows sample's inoculum.","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/percenter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"a percenter function — percenter","text":"","code":"percenter(input = allSamples, parameter = \"Reference\", sampleColumn = 1)"},{"path":"https://cmatkhan.github.io/BSA/reference/percenter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"a percenter function — percenter","text":"input PARAM_DESCRIPTION, Default: allSamples parameter PARAM_DESCRIPTION, Default: 'Reference' sampleColumn PARAM_DESCRIPTION, Default: 1","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/percenter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"a percenter function — percenter","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/percenter.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"a percenter function — percenter","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/percenter2.html","id":null,"dir":"Reference","previous_headings":"","what":"Another percenter function — percenter2","title":"Another percenter function — percenter2","text":"FUNCTION_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/percenter2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Another percenter function — percenter2","text":"","code":"percenter2(input, parameter = \"Reference\", sampleColumn = 1)"},{"path":"https://cmatkhan.github.io/BSA/reference/percenter2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Another percenter function — percenter2","text":"input PARAM_DESCRIPTION parameter PARAM_DESCRIPTION, Default: 'Reference' sampleColumn PARAM_DESCRIPTION, Default: 1","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/percenter2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Another percenter function — percenter2","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/percenter2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Another percenter function — percenter2","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/picker2.html","id":null,"dir":"Reference","previous_headings":"","what":"pick something — picker2","title":"pick something — picker2","text":"Function set comparisons two samples output table way QTLseq package can use. Uses two element list create table can used package QTLseq.","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/picker2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"pick something — picker2","text":"","code":"picker2(SNPset, lowBulk, highBulk)"},{"path":"https://cmatkhan.github.io/BSA/reference/picker2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"pick something — picker2","text":"SNPset List. eelement list table showing data one sample, table set output tabler function. lowBulk String. name sample (element SNPset list) corresponding Low Bulk. highBulk name sample (element SNPset list) corresponding High Bulk.","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/picker2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"pick something — picker2","text":"data.frame format can understood QTLseq.","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/picker2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"pick something — picker2","text":"format used QTLseq package. Picker2 adaptations picker 1 tabler function awk. RealDepth instead realDepth, Alt2 allele.","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/plotMakerBSA.html","id":null,"dir":"Reference","previous_headings":"","what":"plot maker BSA — plotMakerBSA","title":"plot maker BSA — plotMakerBSA","text":"Identifies variants statistically significant x number pools.","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/plotMakerBSA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plot maker BSA — plotMakerBSA","text":"","code":"plotMakerBSA(datum, xplot = datum$binMiddle/1000, yplot = datum$All_pools)"},{"path":"https://cmatkhan.github.io/BSA/reference/plotMakerBSA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plot maker BSA — plotMakerBSA","text":"depth integer. read depth replicate SNP-index calls. altFreq1 numeric. alternate allele frequency bulk . altFreq2 numeric. alternate allele frequency bulk B. replicates integer. number bootstrap replications. filter numeric. optional minimum SNP-index filter","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/plotMakerBSA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"plot maker BSA — plotMakerBSA","text":"Returns vector length replicates delta SNP-indices","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/plotter.html","id":null,"dir":"Reference","previous_headings":"","what":"plot some stuff — plotter","title":"plot some stuff — plotter","text":"function plot enrichment SNP. uses output percenter() input. Obs, set run percenter() well define fileNames outside function, loop make work. mean:  l=percenter(sampleColumn=sampleColumn)","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/plotter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plot some stuff — plotter","text":"","code":"plotter(input, fileName)"},{"path":"https://cmatkhan.github.io/BSA/reference/plotter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plot some stuff — plotter","text":"input PARAM_DESCRIPTION fileName PARAM_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/plotter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"plot some stuff — plotter","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/plotter.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"plot some stuff — plotter","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/runGprimeAnalysis.html","id":null,"dir":"Reference","previous_headings":"","what":"QTLseqr like G prime analysis — runGprimeAnalysis","title":"QTLseqr like G prime analysis — runGprimeAnalysis","text":"Identify QTL using smoothed G statistic","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/runGprimeAnalysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"QTLseqr like G prime analysis — runGprimeAnalysis","text":"","code":"runGprimeAnalysis(   SNPset,   windowSize = 1e+06,   outlierFilter = \"deltaSNP\",   filterThreshold = 0.1,   ... )"},{"path":"https://cmatkhan.github.io/BSA/reference/runGprimeAnalysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"QTLseqr like G prime analysis — runGprimeAnalysis","text":"SNPset Data frame SNP set containing previously filtered SNPs windowSize window size (base pairs) bracketing SNP calculate statitics. Magwene et. al recommend window size ~25 cM, also recommend optionally trying several window sizes test peaks - undersmoothed. Default: 1e+06 outlierFilter one either \"deltaSNP\" \"Hampel\". Method filtering outlier (ie QTL) regions p-value estimation. Default: 'deltaSNP' filterThreshold absolute delta SNP index use filter putative QTL (default = 0.1) ... arguments passed locfit subsequently locfit.raw() (lfproc). Usefull cases get \"vertex space warnings\"; Set maxk higher default 100. See locfit.raw(). getting warning seriously consider increasing window size. Default: 0.1","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/runGprimeAnalysis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"QTLseqr like G prime analysis — runGprimeAnalysis","text":"supplied SNP set tibble G' analysis. Includes five new columns: Gprime - tricube smoothed G statistic based supplied window size pvalue - pvalue SNP calculatd non-parametric estimation negLog10Pval - -Log10(pvalue) supplied quick plotting qvalue - Benajamini-Hochberg adjusted p-value","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/runGprimeAnalysis.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"QTLseqr like G prime analysis — runGprimeAnalysis","text":"wrapper functions perform full G prime analysis identify QTL. following steps performed: 1) Genome-wide G statistics calculated getG.  G defined equation: $$G = 2*\\sum_{=1}^{4} n_{}*ln\\frac{obs(n_i)}{exp(n_i)}$$ SNP, \\(n_i\\) = 1 4 corresponds reference alternate allele depths bulk, described following table: ...\\(obs(n_i)\\) observed allele depths described data frame. getG calculates G statistic using expected values assuming read depth equal alleles bulks: $$exp(n_1) = ((n_1 + n_2)*(n_1 + n_3))/(n_1 + n_2 + n_3 + n_4)$$ $$exp(n_2) = ((n_2 + n_1)*(n_2 + n_4))/(n_1 + n_2 + n_3 + n_4)$$ $$exp(n_3) = ((n_3 + n_1)*(n_3 + n_4))/(n_1 + n_2 + n_3 + n_4)$$ $$exp(n_4) = ((n_4 + n_2)*(n_4 + n_3))/(n_1 + n_2 + n_3 + n_4)$$ 2) G' tricube-smoothed G statistic predicted local regression within chromosome using tricubeStat. works weighted average across neighboring SNPs accounts Linkage disequilibrium (LD) minizing noise attributed SNP calling errors. G values neighboring SNPs within window weighted physical distance focal SNP.  3) P-values estimated based using non-parametric method described Magwene et al. 2011 function getPvals. Breifly, using natural log Gprime median absolute deviation (MAD) calculated. Gprime set trimmed exclude outlier regions (.e. QTL) based Hampel's rule. alternate method filtering QTL proposed using absolute delta SNP indeces greater 0.1 filter potential QTL. estimation mode trimmed set calculated using mlv function package modeest. Finally, mean variance set estimated using median mode p-values estimated log normal distribution.  4) Negative Log10- Benjamini-Hochberg adjusted p-values calculated using p.adjust","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/runQTLseqAnalysis.html","id":null,"dir":"Reference","previous_headings":"","what":"run QTL seq analysis — runQTLseqAnalysis","title":"run QTL seq analysis — runQTLseqAnalysis","text":"FUNCTION_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/runQTLseqAnalysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"run QTL seq analysis — runQTLseqAnalysis","text":"","code":"runQTLseqAnalysis(   SNPset,   windowSize = 1e+06,   popStruc = \"F2\",   bulkSize,   depth = NULL,   replications = 10000,   filter = 0.3,   intervals = c(95, 99) )"},{"path":"https://cmatkhan.github.io/BSA/reference/runQTLseqAnalysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"run QTL seq analysis — runQTLseqAnalysis","text":"SNPset PARAM_DESCRIPTION windowSize PARAM_DESCRIPTION, Default: 1e+06 popStruc PARAM_DESCRIPTION, Default: 'F2' bulkSize PARAM_DESCRIPTION depth PARAM_DESCRIPTION, Default: NULL replications PARAM_DESCRIPTION, Default: 10000 filter PARAM_DESCRIPTION, Default: 0.3 intervals PARAM_DESCRIPTION, Default: c(95, 99)","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/runQTLseqAnalysis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"run QTL seq analysis — runQTLseqAnalysis","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/runQTLseqAnalysis.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"run QTL seq analysis — runQTLseqAnalysis","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/simulateConfInt.html","id":null,"dir":"Reference","previous_headings":"","what":"a simulate function — simulateConfInt","title":"a simulate function — simulateConfInt","text":"FUNCTION_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/simulateConfInt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"a simulate function — simulateConfInt","text":"","code":"simulateConfInt(   SNPset,   popStruc = \"F2\",   bulkSize,   depth = 1:100,   replications = 10000,   filter = 0.3,   intervals = c(0.05, 0.025) )"},{"path":"https://cmatkhan.github.io/BSA/reference/simulateConfInt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"a simulate function — simulateConfInt","text":"SNPset PARAM_DESCRIPTION popStruc PARAM_DESCRIPTION, Default: 'F2' bulkSize PARAM_DESCRIPTION depth PARAM_DESCRIPTION, Default: 1:100 replications PARAM_DESCRIPTION, Default: 10000 filter PARAM_DESCRIPTION, Default: 0.3 intervals PARAM_DESCRIPTION, Default: c(0.05, 0.025)","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/simulateConfInt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"a simulate function — simulateConfInt","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/simulateConfInt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"a simulate function — simulateConfInt","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/simulateSNPindex.html","id":null,"dir":"Reference","previous_headings":"","what":"simulate snp index — simulateSNPindex","title":"simulate snp index — simulateSNPindex","text":"FUNCTION_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/simulateSNPindex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"simulate snp index — simulateSNPindex","text":"","code":"simulateSNPindex(depth, altFreq1, altFreq2, replicates = 10000, filter = NULL)"},{"path":"https://cmatkhan.github.io/BSA/reference/simulateSNPindex.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"simulate snp index — simulateSNPindex","text":"depth PARAM_DESCRIPTION altFreq1 PARAM_DESCRIPTION altFreq2 PARAM_DESCRIPTION replicates PARAM_DESCRIPTION, Default: 10000 filter PARAM_DESCRIPTION, Default: NULL","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/simulateSNPindex.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"simulate snp index — simulateSNPindex","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/simulateSNPindex.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"simulate snp index — simulateSNPindex","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/tabler.html","id":null,"dir":"Reference","previous_headings":"","what":"the tabler — tabler","title":"the tabler — tabler","text":"FUNCTION_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/tabler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"the tabler — tabler","text":"","code":"tabler(VCF, i = 1, minDepth = 5, cutoff = 0.75, filtering = F, cutoff50 = 0.1)"},{"path":"https://cmatkhan.github.io/BSA/reference/tabler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"the tabler — tabler","text":"VCF PARAM_DESCRIPTION integer. Samples vcf file start 10th column. Tabler takes information (9+)th column vcf. Default: 1 minDepth Numeric. minimal depth required position call different allele. Default: 5 cutoff Numeric. Minimum ratio reads (number 0 1) supports either allele required call allele. Default: 0.75. filtering Logical. True, fill column \"filtGenotype\". specially useful single strains, much BSA. function much faster turned . Default: F. cutoff50 PARAM_DESCRIPTION, Default: 0.1","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/tabler.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"the tabler — tabler","text":"OUTPUT_DESCRIPTION","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/tabler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"the tabler — tabler","text":"DETAILS","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/tricubeStat.html","id":null,"dir":"Reference","previous_headings":"","what":"calc tricube stat — tricubeStat","title":"calc tricube stat — tricubeStat","text":":ocal regression (wrapper locfit) predict tricube smoothed version statistic supplied SNP. works weighted average across neighboring SNPs accounts Linkage disequilibrium (LD) minizing noise attributed SNP calling errors. Values neighboring SNPs within window weighted physical distance focal SNP.","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/tricubeStat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"calc tricube stat — tricubeStat","text":"","code":"tricubeStat(POS, Stat, windowSize = 2e+06)"},{"path":"https://cmatkhan.github.io/BSA/reference/tricubeStat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"calc tricube stat — tricubeStat","text":"POS vector genomic positions SNP Stat vector values given statistic SNP WinSize window size (base pairs) bracketing SNP calculate statitics. Magwene et. al recommend window size ~25 cM, also recommend optionally trying several window sizes test peaks - undersmoothed. Default: 2e+06","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/tricubeStat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"calc tricube stat — tricubeStat","text":"Returns vector weighted statistic caluculted tricube smoothing kernel","code":""},{"path":"https://cmatkhan.github.io/BSA/reference/tricubeStat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"calc tricube stat — tricubeStat","text":"DETAILS","code":""},{"path":[]},{"path":"https://cmatkhan.github.io/BSA/reference/tricubeStat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"calc tricube stat — tricubeStat","text":"","code":"df_filt_4mb$Gprime <- tricubeStat(POS, Stat = GStat, WinSize = 4e6) #> Error in tricubeStat(POS, Stat = GStat, WinSize = 4e+06): unused argument (WinSize = 4e+06)"},{"path":"https://cmatkhan.github.io/BSA/news/index.html","id":"bsa-000","dir":"Changelog","previous_headings":"","what":"BSA 0.0.0","title":"BSA 0.0.0","text":"NEW FEATURES Added NEWS.md file track changes package. SIGNIFICANT USER-VISIBLE CHANGES main changes function foo() parameter param. BUG FIXES bug fixes. See details http://bioconductor.org/developers/package-guidelines/#news.","code":""}]
